---
title: "Bayesian Inference on Numerical and Categorical Data"
output: statsr:::statswithr_lab
---

<div class="instructions">
Complete all **Exercises**, and submit answers to **Questions** on the Coursera 
platform.
</div>

## Getting Started

### Load packages

Let's load the necessary packages for this week's lab:

```{r load-packages, message=FALSE}
library(statsr)
library(dplyr)
library(ggplot2)
```

In this week, we will use functions in the `statsr` package to conduct Bayesian inference on numerical variables. The main focus of this lab is to be able to interprete data using results obtained by Bayesian inferences. The data set and analysis functions will be provided by the `statsr` package. And we will be using `dplyr` and `ggplot2` for manipulating and visualizing the data.


### The data

In 2004, the state of North Carolina released a large data set containing 
information on births recorded in this state. These data contain information 
on both the expectant mothers and their children. We will be working with a
random sample of the complete data set. For those of you who took the Inferential
Statistics course as part of the Statistics with R specialization should recognize 
this as the same data set used in the Inference for numerical data lab where we used 
frequentist inference methods to explore these data.

You can load the `nc` data set into our workspace using the `data` function once the `statsr` package is loaded.

```{r load-data}
data(nc)
```

This data set consists of 1000 observations on 13 different variables, some categorical and some 
numerical. The definition of each variable is as follows:

variable         | description
---------------- | ---------------------------------------------
`fage`           | father's age in years.
`mage`           | mother's age in years.
`mature`         | maturity status of mother.
`weeks`          | length of pregnancy in weeks.
`premie`         | whether the birth was classified as premature (premie) or full-term.
`visits`         | number of hospital visits during pregnancy.
`marital`        | whether mother is `married` or `not married` at birth.
`gained`         | weight gained by mother during pregnancy in pounds.
`weight`         | weight of the baby at birth in pounds.
`lowbirthweight` | whether baby was classified as low birthweight (`low`) or not (`not low`).
`gender`         | gender of the baby, `female` or `male`.
`habit`          | status of the mother as a `nonsmoker` or a `smoker`.
`whitemom`       | whether mom is `white` or `not white`.

<br/>



## EDA and Bayesian Inference - `weight`

As a first step in the analysis, we should take a look at the variables in the dataset
and how R has encoded them. The most straight forward way of doing this is using the `str` command:

```{r str}
str(nc)
```

As you review the variable summaries, consider which variables are categorical and which 
are numerical. For numerical variables, are there outliers? If you aren't sure or want to 
take a closer look at the data, make a graph.

<div class="question">
How many of the 13 variables are categorical?

* 5
* 6
* 7
* 8
</div>


We will start with analyzing the weight of the babies at birth, which are contained in the variable `weight`.

<div class="exercise">
Using visualization (e.g., a histogram) and summary statistics (e.g., using `summary` in R), describe the distribution of weight of the babies at birth.
</div>

```{r}
# Type your code for the Exercise 2 here.
```

<div class="question">
Which of the following best describes the distribution of `weight`?

* Left skewed
* Right skewed
* Uniformly distributed
* Normally distributed
</div>

We will use these data to perform basic inference on $\mu$, the average weight of all babies born in North Carolina. Other than the frequentist approach, we will be doing Bayesian inference on $\mu$, which requires us to obtain the posterior distribution of $\mu$. Since it is hard to do this by hand, and the distribution may not be the traditional distribution we can use in R, we will use the `bayes_inference` function in the `statr` package to do simulation. This will allow us construct credible intervals and calculate Bayes factors for a variety of different circumstances. The following is an example how we would use the function `bayes_inference` to construct a credible interval of `weight`.

In order to construct a credible interval of `weight` using `bayes_inference`, we first provide the data (set `data = nc`), and the variable of interest, (set `y = weight`), then indicate that we want a credible interval (`type="ci"`) for a mean (`statistic="mean"`).

```{r}
bayes_inference(y = weight, data = nc, statistic = "mean", type = "ci")
```

In the summary message, we can see that the default the prior of $\mu$ is set to be the improper prior
$$ \pi(\mu~|~\sigma^2) \propto 1, \qquad \pi(\sigma^2)\propto \frac{1}{\sigma^2}, \qquad \Longrightarrow \pi(\mu,\ \sigma^2)\propto \frac{1}{\sigma^2}. $$
And the default credible level is set to be 95%. The credible level for the interval can be specified using the `cred_level = 0.95` argument.

<div class="question">
Which of the following corresponds to the 99% credible interval for the average birth weight of all children born in North Carolina?

* (7.00 , 7.19)
* (6.98 , 7.22)
* (6.94 , 7.26)
* (6.94 , 7.27)
</div>

We can also conduct a Bayesian hypothesis test by calculating Bayes factors. Let us test to see if the average birth weight in North Carolina is significantly different from 7 pounds. Here are the two competing hypotheses:

$$ H_1: \mu = 7 $$
$$ H_2: \mu \ne 7 $$

To conduct this hypothesis test, we will use the `bayes_inference` function with specified argument `type="ht"`. We then also need to provide the `null` and `alternative` arguments to define the null value (7) and the type of alternative hypothesis (`"twosided"`), shown as follows:

```{r}
bayes_inference(y = weight, data = nc, statistic = "mean", type = "ht", null = 7, alternative = "twosided")
```

<div class="question">
Based of Jeffrey's scale for interpretation of a Bayes factors how should be describe the evidence in favor of $H_1$ from the results above?

* Not worth a bare mention
* Positive
* Strong
* Very Strong
</div>

In the graphical results above, $P(H_1 ~|~ data)$ is given by the size of the blue line, and the posterior distribution of $\mu$ under $H_2$ (scaled by $P(H_2 ~|~ data)$) is given via the black curve. The 95% credible interval for $\mu|data,H_2$ is indicated in grey.


<div class="exercise">
In the US, low birth is defined as being less than 2500 grams ($\approx 5.5$ lbs). Use the `bayes_inference` function to assess whether the average birth weight in North Carolina is significantly different from this value. (The answer here should be obvious, but make sure that the Bayes factor result conforms with your intuition.)
</div>

```{r}
# Type your code for the Exercise 2 here.
```


## Inference for two means

Next, let us consider if the mother's smoking habit has any clear effect on the child's birth weight. Here we will use the variable `habit` to distinguish between smoking and non-smoking mothers. Let us first visualized the data to get some understanding of the variable `habit`. 

<div class="exercise">
Construct a side-by-side boxplot of `habit` and `weight` and compare the two distributions. You may use `ggplot` with `geom_boxplot()` to help you.
</div>

```{r normal-ci}
# Type your code for the Exercise 3 here.
```

<div class="question">
Which of the following is *false* about the relationship between `habit` and `weight`?

* Median birth weight of babies born to non-smoker mothers is slightly higher than that of babies born to smoker mothers.
* Range of birth weights of babies born to non-smoker mothers is greater than that of babies born to smoker mothers.
* Both distributions are extremely right skewed.
* The IQRs of the distributions are roughly equal.
</div>

```{r habit-weight-box}
# type your code for the Question 5 here.
```

As before, we use the `bayes_inference` function to either construct a credible interval and or calculate the Bayes factor. The calls will be identical to the single mean case, except now we will provide `habit` as a predictor variable (argument `x = habit`). For credible interval:

```{r}
bayes_inference(y = weight, x = habit, data = nc, statistic = "mean", type = "ci")
```

<div class="question">
Based on the credible interval is there evidence that smoking reduces birth weight? 

* Yes
* No
</div>

For hypothesis test, note that we also change the null value into `null = 0`, since we are interested in comparing if the means of the two groups are equal or not.

```{r}
bayes_inference(y = weight, x = habit, data = nc, statistic = "mean", type = "ht", null = 0, alternative = "twosided")
```

<div class="question">
Based on the Bayes factor calculated above, how strong is evidence against $H_1$?

* Not worth a bare mention
* Positive
* Strong
* Very Strong
</div>


## Inference for proportions

Finally, we can also conduct Bayesian inference when our outcome variable of interest is categorical. Similar to the frequentist inference function we used in the previous courses in this specializatoin, the only change is to specify a $y$ argument that is categorical (with only two levels) and then specify which of its levels is the "success" using the `success` argument. Finally, we change the `statistic` of interest to `"proportion"`.

For example if we want to test if only 7.5% of births in North Carolina are considered low birth weight (variable `lowbirthweight`) we can calculate the Bayes factor using the following code:
```{r}
bayes_inference(y = lowbirthweight, data = nc, success = "low", statistic = "proportion", type = "ht", null = 0.075, alternative = "twosided")
```

<div class="question">
How would the Bayes factor above change if we were to increase the prior probability of $H_2$?

* Get bigger
* Get smaller
* Stay the same
</div>

<div class="question">
How would the Bayes factor above change if we were to change the prior of p to be $Beta(75,925)$?

* Get bigger
* Get smaller
* Stay the same
</div>


Using what you have learned so far, conduct a Bayesian inference procedure to evaluate whether these data provide evidence for or against smoking being associated with low birth weight and smoking being associated with premature birth (variable `premie`).

<div class="question">
These data provide ___________ evidence ___________ smoking affecting the chance of low birth weight.

* weak; for
* strong; for
* weak; against
* strong; against
</div>


<div class="question">
These data provide ___________ evidence ___________ smoking affecting the chance of premature birth.

* weak; for
* strong; for
* weak; against
* strong; against
</div>

## Prediction

A key advantage of Bayesian statistics is prediction and the probabilistic interpretation of predictions.  Much of Bayesian prediction is done using simulation techniques, some of which was discussed near the end of this module. We will go over a simple simulation example.

Suppose you observe four numerical observations of $y$, which are 2, 2, 0 and 0 respectively with sample mean $\bar{y} = 1$ and sample variance $s^2 = 4/3$.  Assuming that $y \sim N(\mu, \sigma^2)$, under the reference prior $p(\mu,\sigma^2) \propto 1/\sigma^2$, our posterior becomes


$$\mu|\sigma^2, y \sim N(1, \sigma^2/4)$$
which is centered at the sample mean and 
$$1/\sigma^2, y \sim Gamma(\alpha = 3/2,\beta = 4/2)$$
where $\alpha = (n - 1)/2$ and $\beta = s^2 (n-1)/2 = 2$.

To obtain the predictive distribution for $y_5$, we can first simulate $\sigma^2$ from its posterior and then $\mu$ followed by $y_5$.  Our draws of $y_5$ will be from the posterior predictive distribution for a new observation.  The example below draws 100,000 times from the posterior predictive distribution of $y_5$.

```{r postpred}
set.seed(314)
N = 100000
phi = rgamma(N,3/2,2)
sigma2 = 1/phi
mu = rnorm(N, 1, sqrt(sigma2/4))
y_5 = rnorm(N, mu, sqrt(sigma2))
```

We can view an estimate of the predictive distribution, by looking at the a smoothed version of the histogram of the simulated data:

```{r preddens}
hist(y_5, prob=T, breaks=30, xlab=expression(y[5]), main="")
```

A 95% central credible interval for a new observation is the interval (L, U) where  $P(Y_5 < L \mid  Y) = .05/2$ and $P(Y_5 > U \mid Y) = .05/2)$.  In this case, since the posterior distribution of $y_5$ is normal, which is symmetric. We can set L to be the 0.025 quantile and U to be the 0.975 quantile.  We can obtain those values using the `quantile` function to find the sample quantiles for 0.025 and 0.975 of $y_5$.

<div class="question">
Estimate a 95\% central credible interval for a new observation $y_5$

* (-3.71, 5.73)
* (-3.11, 5.13)
* (-1.18, 3.19)
</div>

```{r}
# Type your code for Question 12 here.
```


<div class="exercise">
In the simple example above, it is possible to use integration to calculate the posterior predictive analytically.  In this case, it is a scaled $t$ distribution with degree of freedom equal to $(n - 1) = 3$, mean equal to $1$, and scale $s^2(1 + 1/n) = 5/3$.  Plot the empirical density of $y$ alongside the actual density of the t-distribution.  How do they compare?
</div>

```{r tplot}
# Type your code for the Exercise 4 here.
```

